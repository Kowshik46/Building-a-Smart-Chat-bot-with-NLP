{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e0e55d-5f25-4f67-b36d-aefeffe581d3",
   "metadata": {},
   "source": [
    "### Training and Saving Intetn and NER models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9e68b0-d34b-4ea3-bd96-e55387d45527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_intent(data,train_flag):\n",
    "    #Importing the libraries for intent training\n",
    "    import wget\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pickle\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "    #if loop to verify whether the \".txt\" file is present or not, to satisfy the CI/CD pipeline\n",
    "    if (not (os.path.exists('Data/glove.6B.100d.txt'))):\n",
    "        print(\"inesle \")\n",
    "        url= 'https://www.dropbox.com/s/a247ju2qsczh0be/glove.6B.100d.txt?dl=1'\n",
    "        wget.download(url)\n",
    "    #getting the json file keys from the data \n",
    "    keyss = list(data.keys())\n",
    "    array11=np.array(data[keyss[0]])\n",
    "    array12=np.array(data[keyss[1]])\n",
    "    array13=np.array(data[keyss[2]])\n",
    "    array14=np.array(data[keyss[3]])\n",
    "    array15=np.array(data[keyss[4]])\n",
    "    #Merging all the keys into one variable for training and testing\n",
    "    val=np.concatenate([array11, array12,array13,array14,array15])\n",
    "    text=[]\n",
    "    labels=[]\n",
    "    for i in  range(len(val)):\n",
    "        text.append(val[i][0])\n",
    "        labels.append(val[i][1])\n",
    "    train_txt,test_txt,train_label,test_labels = train_test_split(text,labels,test_size = 0.2)\n",
    "    #Implementing bag of words to collect all the words\n",
    "    ls=[]\n",
    "    for c in train_txt:\n",
    "        ls.append(len(c.split()))\n",
    "    maxLen=int(np.percentile(ls, 98))\n",
    "    embeddings_index={}\n",
    "    with open('Data/glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            word=values[0]\n",
    "            coefs=np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word]=coefs\n",
    "    all_embs=np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std= all_embs.mean(), all_embs.std()\n",
    "    max_num_words=40000\n",
    "    embedding_dim=len(embeddings_index['the'])\n",
    "    classes=np.unique(labels)\n",
    "    tokenizer=Tokenizer(num_words=max_num_words)\n",
    "    tokenizer.fit_on_texts(train_txt)\n",
    "    train_sequences=tokenizer.texts_to_sequences(train_txt)\n",
    "    train_sequences=pad_sequences(train_sequences, maxlen=maxLen, padding='post')\n",
    "    test_sequences=tokenizer.texts_to_sequences(test_txt)\n",
    "    test_sequences=pad_sequences(test_sequences, maxlen=maxLen, padding='post')\n",
    "    word_index=tokenizer.word_index\n",
    "    num_words=min(max_num_words, len(word_index))+1\n",
    "    embedding_matrix=np.random.normal(emb_mean, emb_std, (num_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_num_words:\n",
    "            break\n",
    "        embedding_vector=embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    #Initialising One Hot Encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(classes)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoder.fit(integer_encoded)\n",
    "    train_label_encoded = label_encoder.transform(train_label)\n",
    "    train_label_encoded = train_label_encoded.reshape(len(train_label_encoded), 1)\n",
    "    train_label = onehot_encoder.transform(train_label_encoded)\n",
    "    test_labels_encoded = label_encoder.transform(test_labels)\n",
    "    test_labels_encoded = test_labels_encoded.reshape(len(test_labels_encoded), 1)\n",
    "    test_labels = onehot_encoder.transform(test_labels_encoded)\n",
    "    #Dumping the pickle file\n",
    "    with open('Models/utils/tokenizer.pkl', 'wb') as file:\n",
    "        pickle.dump(tokenizer, file)\n",
    "    with open('Models/utils/label_encoder.pkl', 'wb') as file:\n",
    "        pickle.dump(label_encoder, file)\n",
    "    with open('Models/utils/classes.pkl', 'wb') as file:\n",
    "        pickle.dump(classes, file)\n",
    "    if train_flag == 1:\n",
    "        intent_train_model(num_words,train_sequences,train_label,test_sequences,test_labels,embedding_matrix,classes)\n",
    "    return(\"Intent training done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e52105b-c421-41c8-8049-cf0a03a502ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_train_model(num_words,train_sequences,train_label,test_sequences,test_labels,embedding_matrix,classes):\n",
    "    #print(\"in training\")\n",
    "    #Importing libraries for feed forward neural network\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional,Embedding\n",
    "    import pickle\n",
    "    import json\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 100, trainable=False,input_length=train_sequences.shape[1], weights=[embedding_matrix]))\n",
    "    model.add(Bidirectional(LSTM(256, return_sequences=True, recurrent_dropout=0.1, dropout=0.1), 'concat'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(256, return_sequences=False, recurrent_dropout=0.1, dropout=0.1))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(classes.shape[0], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    history = model.fit(train_sequences, train_label, epochs = 15,\n",
    "              batch_size = 20, shuffle=True,\n",
    "              validation_data=[test_sequences, test_labels])\n",
    "    model.save('models/intents.h5')\n",
    "    return(\"Intent training done\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e372ce63-5117-4fee-a98a-197702d2b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner(f): \n",
    "    import spacy\n",
    "    from spacy.tokens import DocBin\n",
    "    from tqdm import tqdm\n",
    "    import json\n",
    "    import os \n",
    "    if (os.path.exists('model-best')):\n",
    "        nlp = spacy.load(\"model-best\") # if already a trained model exists build upon it \n",
    "    else :\n",
    "        nlp = spacy.blank(\"en\") # if not load a blank spacy model\n",
    "    db = DocBin()\n",
    "\n",
    "    TRAIN_DATA = json.load(f)\n",
    "\n",
    "    for text, annot in tqdm(TRAIN_DATA['annotations']): \n",
    "        doc = nlp.make_doc(text) \n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents \n",
    "        db.add(doc)\n",
    "\n",
    "    db.to_disk(\"training_data.spacy\") # save the docbin object\n",
    "\n",
    "    ! python -m spacy init config config.cfg --lang en --pipeline ner --optimize efficiency\n",
    "\n",
    "    ! python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy\n",
    "    \n",
    "    return(\"Training done and New ner is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "343e2842-416a-451f-93e1-14faa913f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kesav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3361: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 8, 100)            12000     \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 8, 512)           731136    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 512)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 256)               787456    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                12850     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,543,697\n",
      "Trainable params: 1,531,697\n",
      "Non-trainable params: 12,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "3/3 [==============================] - 13s 982ms/step - loss: 1.5876 - acc: 0.3333 - val_loss: 1.5726 - val_acc: 0.2000\n",
      "Epoch 2/15\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 1.4079 - acc: 0.3667 - val_loss: 1.3154 - val_acc: 0.4000\n",
      "Epoch 3/15\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 1.2692 - acc: 0.4000 - val_loss: 1.1243 - val_acc: 0.6000\n",
      "Epoch 4/15\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 1.0772 - acc: 0.6000 - val_loss: 1.0728 - val_acc: 0.6000\n",
      "Epoch 5/15\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.9969 - acc: 0.6333 - val_loss: 0.8915 - val_acc: 0.5333\n",
      "Epoch 6/15\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.9134 - acc: 0.6167 - val_loss: 0.6389 - val_acc: 0.8000\n",
      "Epoch 7/15\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.6240 - acc: 0.7333 - val_loss: 0.5370 - val_acc: 0.8000\n",
      "Epoch 8/15\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.5519 - acc: 0.8000 - val_loss: 0.4870 - val_acc: 0.7333\n",
      "Epoch 9/15\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.3503 - acc: 0.8833 - val_loss: 0.3356 - val_acc: 0.8667\n",
      "Epoch 10/15\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.3403 - acc: 0.8333 - val_loss: 0.2687 - val_acc: 0.9333\n",
      "Epoch 11/15\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.1384 - acc: 0.9667 - val_loss: 0.1898 - val_acc: 0.9333\n",
      "Epoch 12/15\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.1555 - acc: 0.9667 - val_loss: 0.1654 - val_acc: 0.8667\n",
      "Epoch 13/15\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.1117 - acc: 1.0000 - val_loss: 0.1259 - val_acc: 0.9333\n",
      "Epoch 14/15\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.1141 - acc: 0.9500 - val_loss: 0.2160 - val_acc: 0.9333\n",
      "Epoch 15/15\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.0492 - acc: 0.9833 - val_loss: 0.1857 - val_acc: 0.9333\n",
      "Intent training done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 58/58 [00:00<00:00, 2416.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[x] The provided output file already exists. To force overwriting the config\n",
      "file, set the --force or -F flag.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 14:12:56.854915: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-08-15 14:12:56.854995: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saving to output directory: .\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     62.94    0.00    0.00    0.00    0.00\n",
      " 50     200         22.30   1324.64  100.00  100.00  100.00    1.00\n",
      "111     400          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "178     600          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "274     800          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "374    1000          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "474    1200          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "663    1400          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "863    1600          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1063    1800          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "[+] Saved pipeline to output directory\n",
      "model-last\n",
      "Training done and New ner is ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 14:13:10.385476: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-08-15 14:13:10.385555: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[2022-08-15 14:13:21,776] [INFO] Set up nlp object from config\n",
      "[2022-08-15 14:13:21,804] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-08-15 14:13:21,814] [INFO] Created vocabulary\n",
      "[2022-08-15 14:13:21,817] [INFO] Finished initializing nlp object\n",
      "[2022-08-15 14:13:22,056] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    }
   ],
   "source": [
    "#training all the models and save them \n",
    "\n",
    "import json\n",
    "with open('Data/CW11.json') as file:\n",
    "    data=json.loads(file.read())\n",
    "train_flag=1\n",
    "print(train_intent(data,train_flag))#training intent file \n",
    "f = open('Data/version4.json')\n",
    "print(train_ner(f)) # traingin ner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29126f-b14f-4e58-9160-654484f95fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
